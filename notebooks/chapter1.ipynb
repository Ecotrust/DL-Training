{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ecotrust/DL-Training/blob/main/notebooks/chapter1.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes from Chapter 1 of the book \"Modern computer vision with pytorch\" by V. Kishore Ayyadevara and Yeshwanth Reddy (2020).\n",
        "\n",
        "This notebook was modified from the original version available [here](https://github.com/PacktPublishing/Modern-Computer-Vision-with-PyTorch/blob/master/Chapter01/Chain_rule.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BH_Y4RhiQzB"
      },
      "source": [
        "# Artificial Neural Networks \n",
        "\n",
        "Artificial neural networks (ANNs) are a class of machine learning algorithms inspired by the human brain's biological neural networks (BNNs). BNNs are composed of billions of neurons connected to each other, forming a powerful processing unit that allows us to interact with our environment, learn from it, and adapt to it. As depicted in the figure below, our sensory system collects and encodes information from the environment and sends it to the brain, where it is processed and evaluated against prior experiences to trigger appropriate responses.\n",
        "\n",
        "<fugure>\n",
        "    <img src=\"figures/fig21_pandya_and_macy.jpg\" width=\"500\">\n",
        "    <figcaption>From A.S. Pandya and R.B. Macy. 1996. Pattern recognition with neural networks in C++. CRC Press, Florida.</figcaption>\n",
        "</figure>\n",
        "\n",
        "ANNs aims to replicate how individual neurons process and transfer information in the brain. Neurons are the brain's basic processing units, composed of a cell body, dendrites, and an axon. The cell body contains the nucleus and the cytoplasm, the site of the neuron's metabolic activity. Dendrites are the input structures of the neuron, and they receive signals from other neurons. The axon is the output structure of the neuron, and it transmits signals to other neurons. If you want to learn more about how neurons communicate with each other, you can check out this [video](https://youtu.be/OvVl8rOEncE).\n",
        "\n",
        "The artificial neuron is a computational model that emulates some properties of biological neurons (see figure below). The simplest neuron architecture is composed of a set of inputs (analogous to dendrites), a set of weights (analogous to synapses),  and an activation function (analogous to the neuron action potential mechanism). The inputs are signals/data fed to the neurons from an input layer or other neurons. The weights modulate the importance of the information, analogous to the strength of the synapsis connecting two neurons. The activation function is a non-linear function that determines the neuron's output. \n",
        "\n",
        "<fugure>\n",
        "    <img src=\"figures/neuron_perceptron.svg\" width=\"1000\">\n",
        "    <figcaption>Modified from <a href=\"https://www.khanacademy.org/test-prep/mcat/organ-systems/neural-synapses/a/signal-propagation-the-movement-of-signals-between-neurons\">Khan Academy</a></figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "# Deep Learning Fundamentals\n",
        "\n",
        "Modern deep learning algorithms involve two main subroutines: \n",
        "- **Feedforward propagation**: During this stage, the data is feed into the model and the output and an error associated with it are computed. A series of operations are performed on the data to extract features, that is patterns or \"motifs\" that are useful to predict a given output, such as faces or objects, edge detection, etc.\n",
        "\n",
        "- **Backpropagation**: Consist of finding the set of parameters that minimizes the error between model predictions and the input data. This is done by computing the gradient of the error function with respect to the parameters and updating the parameters in the opposite direction of the gradient. This is done iteratively until the error is acceptable for the task at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feedforward propagation\n",
        "\n",
        "There are three steps involved in feedforward propagation: pre-activation, activation, and post-activation. During pre-activation, the hidden layer is obtained by computing the product of inputs and weights (dot product for matrices). During activation, we apply a non-linear activation function $f$ to the hidden layer. Finally, the post-activation consist in the computation of the sum of products of the hidden layer activation values and the weights to produce the output $\\hat y$. A cost function is then used to compute the prediction error or loss. \n",
        "\n",
        "The output is typically calculated as follows:\n",
        "\n",
        "$\\hat y = f(b_i + \\sum_{i=1}^{n} w_ix_i)$\n",
        "\n",
        "where \n",
        "\n",
        "$\\hat y$ is the output or prediction outcome as a continuous or discrete variable.\n",
        "\n",
        "$f$ is the activation function.\n",
        "\n",
        "$b$ is the bias term.\n",
        "\n",
        "$w_i$ are the weights that link input, hidden, and output layers.\n",
        "\n",
        "$x_i$ are the input values. \n",
        "\n",
        "As an example, consider a network with two inputs, one hidden layer with three neurons, and one output as shown in the figure below. \n",
        "\n",
        "<fugure>\n",
        "    <img src=\"figures/3neuron_arch.png\" width=\"500\">\n",
        "    <figcaption>Modified from figure in page 17, Ch.1</figcaption>\n",
        "</figure>\n",
        "\n",
        "To think in our network architecture in more concrete terms, lets add some synthetic data. \n",
        "\n",
        "$x = \\begin{pmatrix} 1 & 1 \\end{pmatrix}$\n",
        "\n",
        "$w^h = \\begin{pmatrix}-0.0053 & -0.582 & -0.2723\\\\\\ 0.3793 & -0.5204 & 0.1896 \\end{pmatrix}$ \n",
        "\n",
        "$w^o = \\begin{pmatrix}0.1528\\\\\\ -0.1745\\\\\\ -0.1135 \\end{pmatrix}$\n",
        "\n",
        "$b^h = \\begin{pmatrix} -0.0140 & 0.5607 & -0.0628 \\end{pmatrix}$\n",
        "\n",
        "$b^o = \\begin{pmatrix} -0.5516 \\end{pmatrix}$\n",
        "\n",
        "$y = 0$\n",
        "\n",
        "Our input layer is $x$, $w^h$ is the weight matrix connecting the input and hidden layers, $b^h$ are the bias associated with the hidden layer, $w^o$ are the weights connecting the hidden layer and the output, $b^o$ is the bias associated with the output, and $y$ is a continuous variable with the observation we want to predict. The first pass of feedforward propagation uses an arbitrary selection (usually a random sample) of weights and bias. \n",
        "\n",
        "Note that $w^h$ have 2(inputs) x 3(neurons) = 6 weights and $w^o$ has 3(neurons) = 3 weights. \n",
        "\n",
        "Let's implement each step of the feedforward propagation in python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.36  -0.542 -0.145]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np \n",
        "\n",
        "# Set x, w (weights for input and hidden layers), and w0 (bias for input and hidden layers)\n",
        "x = np.array([[1, 1]])\n",
        "w_h = np.array([[-0.0053, -0.582 , -0.2723],\n",
        "                [ 0.3793, -0.5204,  0.1896]], dtype=np.float32)\n",
        "w_o = np.array([[ 0.1528], [-0.1745], [-0.1135]], dtype=np.float32)\n",
        "b_h = np.array([-0.0140, 0.5607, -0.0628], dtype=np.float32)\n",
        "b_o = np.array([-0.5516], dtype=np.float32)\n",
        "\n",
        "# Compute hidden layer\n",
        "h = (x @ w_h) + b_h\n",
        "print(np.round(h,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activation function \n",
        "\n",
        "The activation function allows to introduce non-linearity into the model. It determines which neurons are activated (i.e. pass a non-zero value to the next layer) and which are set to zero. The most common activation functions are the rectified linear unit or ReLU, tanh, and sigmoid.\n",
        "\n",
        "Let's apply a sigmoid activation, defined as follows:\n",
        "\n",
        "$f(x) = \\frac{1}{1 + e^{-x}}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f(h) = [[0.589 0.368 0.464]]\n"
          ]
        }
      ],
      "source": [
        "# Compute activation. One value for each node in the hidden layer.\n",
        "f_h = 1/(1 + np.exp(-h))\n",
        "print(\"f(h) =\", np.round(f_h, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After activation, we can compute the network output y_hat. We use the same procedure used to compute the hidden layer, but this time we calculate the dot product of the activation and the weights associated with the output layer plus the output bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_hat = [[-0.578]]\n"
          ]
        }
      ],
      "source": [
        "# Compute output value y_hat\n",
        "y_hat = (f_h @ w_o) + b_o\n",
        "print(\"y_hat =\", np.round(y_hat, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss function\n",
        "\n",
        "The loss function is used to compute the error between the model prediction and the observed value. There are many ways to estimate the loss. A widely used loss function for continuos problems (e.g. predicting housing prices) is the mean squared error (MSE), which is defined as: \n",
        "\n",
        "$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
        "\n",
        "where\n",
        "\n",
        "$y_i$ is the observed value.\n",
        "\n",
        "$\\hat{y}_i$ is the predicted value.\n",
        "\n",
        "Another common loss function is the cross-entropy loss, which is used for classification problems (e.g. land use/cover classification). It is defined as:\n",
        "\n",
        "$CE = -\\frac{1}{n}\\sum_{i=1}^{n}y_i\\log(\\hat{y}_i)$\n",
        "\n",
        "For this example we use the MSE loss function. \n",
        "\n",
        "The observed value is y = 0 and n = 1 because we have only one prediction, thus \n",
        "\n",
        "$MSE = \\frac{1}{1}(0 - (-0.578))^2 = (0.578)^2$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss = [[0.335]]\n"
          ]
        }
      ],
      "source": [
        "# Compute loss\n",
        "loss = np.square(y_hat)\n",
        "print('loss =', np.round(loss, 3)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7mc04emjTCa"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "During backpropagation, the weights are adjusted to minimize the error (loss). The process consists of adjusting the weights one at a time by a small amount.\n",
        "\n",
        "Steps:\n",
        "\n",
        "1. Change one weight at a time by adding a small amount to it.\n",
        "2. Calculate the change in the loss induced by this change.\n",
        "3. Update the weight by $-k \\frac {\\delta L}{\\delta W}$, where $k$ is the learning rate and $\\frac {\\delta L}{\\delta W}$ is the change in the loss induced by the change in the weight, aka the gradient.\n",
        "4. Repeat steps 1-3 n *epochs* or iterations over the entire dataset.\n",
        "\n",
        "<fugure>\n",
        "    <img src=\"figures/3neuron_arch_bp.png\" width=\"500\">\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To see the steps involved in backpropagation more clearly, let's update only one weight from $w_h$, say $w^h_{11}$. Later on we will define some methods to perform the operations automatically on all weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original weight: -0.0053 \n",
            "updated weight = -0.00102 \n",
            "original y_hat = [[-0.5784]] \n",
            "new y_hat = [[-0.57824]] \n",
            "gradient = [[-0.043]] \n",
            "original loss = [[0.335]] \n",
            "new loss = [[0.334]]\n"
          ]
        }
      ],
      "source": [
        "# Copy w_h to preserve the original weights for later use\n",
        "from copy import deepcopy\n",
        "w_h_tmp = deepcopy(w_h) \n",
        "w_h_updated = deepcopy(w_h)\n",
        "\n",
        "# Change the first weight in w_h by an arbitrary small amount.\n",
        "w_h_tmp[0][0] = w_h[0][0] + 0.0001\n",
        "\n",
        "# Re-compute the loss with the modified weight\n",
        "# We need to re-compute all feedforward propagation steps\n",
        "# although in this case we use the temporal weight matrix w_h_tmp\n",
        "h = (x @ w_h_tmp) + b_h\n",
        "f_h = 1/(1 + np.exp(-h))\n",
        "tmp_y_hat = (f_h @ w_o) + b_o\n",
        "tmp_loss = np.square(tmp_y_hat)\n",
        "\n",
        "# Compute the gradient of the loss with respect to the first weight in w_h\n",
        "grad = (tmp_loss - loss) / 0.0001\n",
        "\n",
        "# Calculate new weight using a learning rate of 0.1\n",
        "w_h_updated[0][0] = w_h[0][0] - 0.1 * grad\n",
        "\n",
        "# Re-compute the loss with the updated weight\n",
        "h = (x @ w_h_updated) + b_h\n",
        "f_h = 1/(1 + np.exp(-h))\n",
        "new_y_hat = (f_h @ w_o) + b_o\n",
        "new_loss = np.square(new_y_hat)\n",
        "\n",
        "print(\n",
        "    'original weight:', np.round(w_h[0][0], 4),\n",
        "    '\\nupdated weight =', np.round(w_h_updated[0][0], 5),\n",
        "    '\\noriginal y_hat =', np.round(y_hat, 5),\n",
        "    '\\nnew y_hat =', np.round(new_y_hat, 5),\n",
        "    '\\ngradient =', np.round(grad, 3),\n",
        "    '\\noriginal loss =', np.round(loss, 3),\n",
        "    '\\nnew loss =', np.round(new_loss, 3)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the results above, note that new_loss < loss and that the new prediction new_y_hat also improved slightly over y_hat. By appliying the same steps on all weights and bias, we can iteratively improve the model until the loss is acceptable. An **epoch** is a complete pass over the entire dataset, where all elements in $w^h$, $w^o$, $b^h$, and $b^o$ are adjusted.\n",
        "\n",
        "Let's see the effect of adjusting weights and bias over multiple epochs. To make the process more efficient, we will define one function to perform feedforward and one to perform backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:23:27.984719Z",
          "start_time": "2020-09-24T13:23:27.929962Z"
        },
        "id": "i5WpgCBbm_Jc"
      },
      "outputs": [],
      "source": [
        "def feed_forward(x, y, W):\n",
        "    '''Implements feedfordward propagation from scratch.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x: array\n",
        "      Input layer <- training data\n",
        "    y: array\n",
        "      Observations <- this is what we want to predict.\n",
        "    W: list\n",
        "      Weights and bias for input, hidden, and output layers.\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      Loss as the Mean Square Error (MSE).\n",
        "    '''\n",
        "    # Pre-activation\n",
        "    # Calculate hidden layer as the product w_i*x_i (dot product for matrix multiplication) plus the bias.\n",
        "    # W[0] = w_h, the weights connecting input layer with hidden layer. \n",
        "    # W[1] = b_h, the bias associated with the hidden layer.\n",
        "    h = np.dot(x, W[0]) + W[1]\n",
        "    \n",
        "    # Activation\n",
        "    # Apply f as a sigmoid activation function.\n",
        "    f_h = 1/(1+np.exp(-h))\n",
        "\n",
        "    # Post-activation\n",
        "    # Calculate the output layer (a)\n",
        "    # W[2] = w_o, the weights connecting the hidden layer with the output layer. \n",
        "    # W[3] = b_o, the bias associated with the output layer.\n",
        "    y_hat = np.dot(f_h, W[2]) + W[3]\n",
        "    \n",
        "    # Calculate the loss as the MSE \n",
        "    mean_squared_error = np.mean(np.square(y - y_hat))\n",
        "\n",
        "    return mean_squared_error\n",
        "\n",
        "\n",
        "def update_weights(x, y, W, k):\n",
        "    '''Implements gradient descent from scratch.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    x: array\n",
        "      Input layer with training data\n",
        "    y: array\n",
        "      Observations we want to predict\n",
        "    W: list \n",
        "      Weights and bias for input, hidden, and output layers\n",
        "    k: float\n",
        "      Learning rate\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    (list, list)\n",
        "      Tuple with list of updated weights and list of original weights\n",
        "    '''\n",
        "    # Create copies of the original weights list.\n",
        "    original_weights = deepcopy(W)\n",
        "    temp_weights = deepcopy(W)\n",
        "    updated_weights = deepcopy(W)\n",
        "\n",
        "    # Calculate loss with original set of weights.\n",
        "    original_loss = feed_forward(x, y, original_weights)\n",
        "\n",
        "    # Iterate through each weight in W. \n",
        "    for i, layer in enumerate(original_weights):\n",
        "        for index, weight in np.ndenumerate(layer):\n",
        "            temp_weights = deepcopy(W)\n",
        "            # Update one weight at a time by a small amount.\n",
        "            temp_weights[i][index] += 0.0001\n",
        "            # Re-calculate loss with updated weight.\n",
        "            _loss_plus = feed_forward(x, y, temp_weights)\n",
        "            # Calculate the gradient of the loss with respect to the weight.\n",
        "            grad = (_loss_plus - original_loss)/(0.0001)\n",
        "            # Reduce the weight in proportion to the gradient multiplied by the learning rate (k).\n",
        "            updated_weights[i][index] -= k*grad\n",
        "\n",
        "    return updated_weights, original_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use the same data as previously, but for convenience we'll pack it to feed it to the functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:23:28.000882Z",
          "start_time": "2020-09-24T13:23:27.991191Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp9I03hRnh2G",
        "outputId": "1f911858-3eb9-4b19-c281-9c46f845c48b"
      },
      "outputs": [],
      "source": [
        "# Observations\n",
        "y = np.array([[0]])  \n",
        "# Pack all weights and bias into a list \n",
        "W = [w_h, b_h, w_o, b_o]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Loss over increasing number of epochs')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqj0lEQVR4nO3deXxV9Z3/8dfnZieEAEnYwhYWFVRETAGFYq1LxWXQLlO1rbVqKb/R2m2mY5fpdGo7tjNtH3Y6LqXWOk6r1IfVkSpu1SouFQgWlFXCHtnCvmb//P44J3iJCbmBJDc59/18PO4jZ/l+z/l8z7353HO/ZzN3R0REoiuW7ABERKRjKdGLiEScEr2ISMQp0YuIRJwSvYhIxCnRi4hEnBK9tCszO2hmI5IdRyLM7Ntmdn+y4zhZZuZmNipJ6z7VzP5mZgfM7LZkxNBUMrdHV5We7ACizMw2ADe7+5+THUtncfeeyY4hUe7+78mOIQK+Cbzs7mcnOxBpmfbo5YSYWaftJHTmulLZCW7nYcDy9o5F2pcSfRKYWZaZ3WVmW8LXXWaWFc4rNLOnzGyvme02s1fNLBbO+2czey/8mbzazC5sYfn5ZvaQmVWa2UYz+66ZxcL17jWzM+LKFpnZETPrF45fYWZLwnJvmNm4uLIbwhjeBg41lxjifzab2YNmdreZPR3GvMDMRsaVPd3MXgjbud3Mvh1O/76ZPWZmvzOz/cANYZt+Y2Zbw23wQzNLC8uPNLOXzGyXme00s9+bWe+49TS73cL1/C4cHh7G/nkz2xQu5ztxy8gxs/8xsz1mttLMvmlmFcd5j93MZpnZmrDO3WZmTdfbZN3p4fjLYfveCLvC/mRmBWG79pvZIjMb3mSVl5nZujDu/2z8zITLuzGMeY+ZPWdmw5rEeYuZrQHWtNCWvzOz5eFn4mUzGxNOfwm4APjvMM5Tmql7vPftBjN73cx+aWb7zGxV/GfazAaZ2dzw81FuZl+Mm5dmQdfb2vB9XWxmQ+JWfVEL236Umb0Srm+nmf2hpfcwUtxdrw56ARuAi5qZ/gPgTaAfUAS8AdwRzrsTuA/ICF8fBgw4FdgMDArLDQdGtrDeh4Angbyw3LvATeG8B4AfxZW9BXg2HJ4A7AAmAWnA58M2ZMW1ZwkwBMhpYd0OjAqHHwR2AxMJugl/D8wJ5+UBW4FvANnh+KRw3veBWuAqgp2RHOD/gF8BueF2Wwh8KSw/CrgYyAq353zgrnBei9stXM/v4qY78OtwfWcB1cCYcP6PgVeAPsBg4G2g4jjvvQNPAb2BoUAlcGnT9TZZd3o4/jJQDowE8oEV4Xt4UbgdHwJ+22RdfwH6hut6l6DLkHAblgNjwrrfBd5oUveFsO4H3lPgFOBQuH0zCLpqyoHMuFhvPs52ON77dgNQB3wtXPangX1A33D+K8A9BJ+P8eE2vDCc90/AO+H7a+H7VZDAtn8E+A7B5yobmJrsPNEpuSjZAUT5RcuJfi1wWdz4x4AN4fAPCJL0qCZ1RhEk4YuAjOOsM40gQY2Nm/Ylgn5Uwvrr4ua9DlwfDt9L+IUTN381cH5ce25spc1NE/39cfMuA1aFw9cCf2thGd8H5seN9w/blBM37VrgLy3Uv6px2cfbbjSf6AfHzV8IXBMOrwM+FjfvZlpP9FPjxh8Fbm+63ibrjk/034mb/zPgmbjxK4ElTdZ1adz4PwAvhsPPEH7Jh+Mx4DAwLK7uR4/Tjn8BHm1S/z3gI3GxNpvoW3vfCBL9FsCabPPPEexM1AN5cfPuBB6M+1zOOIFt/xAwO/59ToWXum6SYxCwMW58YzgN4D8J9pieD3+K3w7g7uXAVwmSxA4zm2Nmg/igQiCzmeUXh8MvATlmNin8CT8eeCKcNwz4RvgTfa+Z7SX4h4tfz+Y2tnVb3PBhoPFg7RCCL7yWxK9nGMEe39a4uH5FsIeImfULt8d7YVfP7wi2Q1u2W2vxDmoSUyLboaVlJWJ73PCRZsabLis+nvjP0zDgF3HbbTfBHnBxC3WbOuaz6u4NYfniFmu877jvW+g9DzNwk9gHAbvd/UCTeY3rbe3z09K2/yZB+xeG3VE3JtCObk+JPjm2EPwTNBoaTsPdD7j7N9x9BMGe29cb+y3d/WF3nxrWdeAnzSx7J0G3R9Plvxcuo4FgD+da4Drgqbh/ps0E3Tq941493P2RuGW11+1ONxN0TbQkfj2bCfYMC+Pi6uXup4fz7wzLj3P3XsBnCf6ZgwUltt1as5Wgy6bRkJYKJuAQ0CNufMBJLKtRfDxHP08E2+5LTd7THHd/I6788d7TYz6rYV/3EMLPUytae98Aihv7z5vEvgXoa2Z5TeY1rre1z0+z3H2bu3/R3QcR/NK9x1LgVEwl+o6XYWbZca90gn7C71pwILQQ+B7BXmjjwdBR4Yd/P8HP13oLzlf+qAUHbasI9urqm67M3esJEvmPzCwv3Gv/euPyQw8T9Id+Jhxu9GtgVri3b2aWa2aXN/lnay9PAQPM7KsWHCTOM7NJzRV0963A88DPzKyXBQeWR5rZ+WGRPOAgsNfMign6b4Gj53m3ut0S8CjwLTPrE67j1hNYRqMlwDQzG2pm+cC3TmJZjf4pjG0I8BWg8SDjfQRxnw5HD45+qg3LfRS43MwuNLMMgmMq1QTHlY4rgfcNgr3728wsI4xrDDDP3TeH67gz/L8ZB9xEcJwH4H7gDjMbHX5Wx5lZQWsxmdmnzKzxC3sPwZfciXweuhUl+o43jyC5NL6+D/wQKCM4oPcO8FY4DWA08GeCxPVX4B53f5ngQOOPCfbYtxH8g3y7hXV+mWCvcR3wGkEyf6BxprsvCOcPIujDbZxeBnwR+G+Cf4Jygn7Udhf+iriY4FfLNoIzPi44TpXrCbqkVoSxPQYMDOf9G8GB5H3A08DjcfXast2O5wdABbCe4P15jCDhtZm7v0CQiN8GFhN86Z2sJ8NlLSHYBr8J1/UEwS+YOWG31jJgehtiXU3wC+mXBNvwSuBKd69JcBHHe98AFhB85ncCPwI+6e67wnnXEhy/2ELQvfiv4bYD+DnBl9DzBDtEvyE4iN6aDwELzOwgMBf4iruvT7At3ZYd2z0mIokws/9HcKD2/FYLS7PM7AaCA7lTkx1L1GmPXiQBZjbQzKaE3Q+nEnRhPNFaPZGuQFcciiQmk+CMkRJgLzCH4BxvkS4voa4bM7sU+AXBOdr3u/uPm8yfAdwBNBBcAPFVd38tnLcBOEBwwKPO3UvbswEiInJ8rSZ6Cy5XfpfgwFkFsAi41t1XxJXpCRxydw+Pjj/q7qeF8zYApe6+s2OaICIix5NI181EoNzd1wGY2RxgBsFRdADc/WBc+VxO8lzrwsJCHz58+MksQkQkpSxevHinuxc1Ny+RRF/MsVfOVRDcC+UYZnY1wYUr/YDL42Y5wVWeDvzK3Wc3txIzmwnMBBg6dChlZWUJhCYiIgBmtrGleYmcdWPNTPvAHru7PxF211xF0F/faIq7TyA4d/cWM5vW3Ercfba7l7p7aVFRs19KIiJyAhJJ9BUce3n1YN6/vPoD3H0+MDK84hN3b7y0fwfB6WgTTzhaERFps0QS/SJgtJmVmFkmcA3BFWVHxV2yj5lNIDgVbVd4CX1eOD0XuITgyjwREekkrfbRu3udmd0KPEdweuUD7r7czGaF8+8DPgFcb2a1BJf5fzo8A6c/8ET4HZAOPOzuz3ZQW0REpBld8hYIpaWlroOxIiKJM7PFLV2npFsgiIhEnBK9iEjERSbRV9fVc98ra3l1TWWyQxER6VIik+gz02LMnr+OJ5e0eOaniEhKikyiNzPOGdaHsg27kx2KiEiXEplED1A6rA8bdh2m8sAJPfhHRCSSopXoh/cBYPHGPUmORESk64hUoj+jOJ/M9BiLN6r7RkSkUaQSfVZ6GuOK8ynTHr2IyFGRSvQA5wzvw7L39lFVW5/sUEREuoTIJfoPDetLbb2zdPPeZIciItIlRC7RnzMsOCCr7hsRkUDkEn2f3ExGFuXqzBsRkVDkEj1A6bC+LN64h4aGrndnThGRzhbJRH/O8D7sO1JLeeXB1guLiERcJBN9aWM//QZ134iIRDLRlxTmUpCbqfveiIgQ0URvZkws6cuC9Ur0IiKRTPQAk0cU8N7eI2zefTjZoYiIJFVkE/2kEX0BeHPdriRHIiKSXJFN9Kf0y6NPjwx134hIykso0ZvZpWa22szKzez2ZubPMLO3zWyJmZWZ2dRE63aUWCzop9cevYikulYTvZmlAXcD04GxwLVmNrZJsReBs9x9PHAjcH8b6naYySMKqNhzhIo96qcXkdSVyB79RKDc3de5ew0wB5gRX8DdD7p742WouYAnWrcjTSopAGDBOnXfiEjqSiTRFwOb48YrwmnHMLOrzWwV8DTBXn3CdcP6M8Nun7LKyspEYm/VaQPyyM/JYMF6dd+ISOpKJNFbM9M+cBMZd3/C3U8DrgLuaEvdsP5sdy9199KioqIEwmrd+/302qMXkdSVSKKvAIbEjQ8GtrRU2N3nAyPNrLCtdTvC5BEFbNp9mC17j3TmakVEuoxEEv0iYLSZlZhZJnANMDe+gJmNMjMLhycAmcCuROp2tEklwfn06r4RkVTVaqJ39zrgVuA5YCXwqLsvN7NZZjYrLPYJYJmZLSE4y+bTHmi2bge0o0VjBvaiV3Y6b65V942IpKb0RAq5+zxgXpNp98UN/wT4SaJ1O1NazJg0ooA31u1MVggiIkkV2Stj400dVcjm3UfYtEvn04tI6kmJRD9lVCEAr5Vrr15EUk9KJPqRRbkM6JXN60r0IpKCUiLRmxlTRhXy+tqdeo6siKSclEj0AB8eXcjew7Ws2Lo/2aGIiHSqlEn0540K7nujfnoRSTUpk+j75WVzav889dOLSMpJmUQPwdk3C9fvpqq2PtmhiIh0mpRK9FNHF1Bd18BbG/ckOxQRkU6TUol+YkkB6TFTP72IpJSUSvQ9s9I5e2hvXl2jRC8iqSOlEj3AtNFFvPPePnYerE52KCIinSLlEv1HTu0HwPx32+cpViIiXV3KJfrTB/WisGcmL69WoheR1JByiT4WM6adUsT8NZXU63YIIpICUi7RQ9B9s/dwLUsr9iY7FBGRDpeSiX7a6EJihrpvRCQlpGSi790jk/FDevPK6h3JDkVEpMOlZKKHoPvm7ff2sUunWYpIxKVwoi/CHeavUfeNiERbQonezC41s9VmVm5mtzcz/zNm9nb4esPMzoqbt8HM3jGzJWZW1p7Bn4wzBuXrNEsRSQnprRUwszTgbuBioAJYZGZz3X1FXLH1wPnuvsfMpgOzgUlx8y9w9y5134HG0yxfWrWDuvoG0tNS9seNiERcItltIlDu7uvcvQaYA8yIL+Dub7h74y0h3wQGt2+YHePiMf3Ze7iWxbqbpYhEWCKJvhjYHDdeEU5ryU3AM3HjDjxvZovNbGZLlcxsppmVmVlZZWXndKd8+JQiMtNi/Hnl9k5Zn4hIMiSS6K2Zac1eUmpmFxAk+n+OmzzF3ScA04FbzGxac3Xdfba7l7p7aVFRUQJhnbyeWemcO7KAF1Zsx11XyYpINCWS6CuAIXHjg4EtTQuZ2TjgfmCGu+9qnO7uW8K/O4AnCLqCuoyLxvZnw67DrK08mOxQREQ6RCKJfhEw2sxKzCwTuAaYG1/AzIYCjwOfc/d346bnmlle4zBwCbCsvYJvDxeNCe5m+cIKXTwlItHUaqJ39zrgVuA5YCXwqLsvN7NZZjYrLPY9oAC4p8lplP2B18xsKbAQeNrdn233VpyEgfk5nFmcr356EYmsVk+vBHD3ecC8JtPuixu+Gbi5mXrrgLOaTu9qLhrTn7tefJedB6sp7JmV7HBERNqVTh4HLhrbD3d4aaW6b0QkepTogbEDezEoP5sX1H0jIhGkRA+YGReP7c+rayo5XFOX7HBERNqVEn3o0jMGUlXboHvfiEjkKNGHJpb0pSA3k3nvbE12KCIi7UqJPpQWMy45fQAvrdpBVW19ssMREWk3SvRxLjtzAIdr6nnlXXXfiEh0KNHHmTyigN49Mnh22bZkhyIi0m6U6ONkpMW4ZGx//rxiO9V16r4RkWhQom9i+pkDOVBdx+vlXeo5KSIiJ0yJvokpIwvJy05n3jvqvhGRaFCibyIzPcbFY/rz/PJt1NQ1JDscEZGTpkTfjCvPGsT+qjrm6+wbEYkAJfpmTB1dSJ8eGTy59APPVxER6XaU6JuRkRbjsjMH8sKKbRyq1r1vRKR7U6JvwYzxxVTVNvDCCt3RUkS6NyX6FpQO68Og/GyeXPJeskMRETkpSvQtiMWMK8cPYv6anew6WJ3scERETpgS/XHMOKuY+gZnnm6JICLdmBL9cYwZmMfofj350xKdfSMi3ZcS/XGYGTPGD2Lhht1s3n042eGIiJyQhBK9mV1qZqvNrNzMbm9m/mfM7O3w9YaZnZVo3a7u6gmDMYPH39JBWRHpnlpN9GaWBtwNTAfGAtea2dgmxdYD57v7OOAOYHYb6nZpxb1zOG9kAY+9tZmGBk92OCIibZbIHv1EoNzd17l7DTAHmBFfwN3fcPc94eibwOBE63YHnzxnMJt3H2Hhht3JDkVEpM0SSfTFwOa48YpwWktuAp5pa10zm2lmZWZWVlnZte4xc+npA+mZlc5jiyuSHYqISJslkuitmWnN9mGY2QUEif6f21rX3We7e6m7lxYVFSUQVufJyUzjinEDmffOVt0SQUS6nUQSfQUwJG58MPCB8w3NbBxwPzDD3Xe1pW538MlzBnO4pp5572xNdigiIm2SSKJfBIw2sxIzywSuAebGFzCzocDjwOfc/d221O0uzhnWh5LCXHXfiEi302qid/c64FbgOWAl8Ki7LzezWWY2Kyz2PaAAuMfMlphZ2fHqdkA7OpyZ8clzBrNg/W427DyU7HBERBJm7l3vlMHS0lIvKytLdhgfsG1fFVN+8hI3f7iEb00fk+xwRESOMrPF7l7a3DxdGdsGA/KzufC0fjxWVqHHDIpIt6FE30bXTRrKrkM1PLdcNzoTke5Bib6Npo0uYnCfHH6/YGOyQxERSYgSfRvFYsa1E4fy5rrdlO84mOxwRERapUR/Aj5VOpj0mPHIwk3JDkVEpFVK9CegX142Hzt9AH98q4Kq2vpkhyMiclxK9CfoM5OGsvdwLX9a2i0v9BWRFKJEf4LOHVnAKf178tvXN9AVr0UQEWmkRH+CzIwbzithxdb9LNqwp/UKIiJJokR/Eq4+u5j8nAx++/r6ZIciItIiJfqTkJOZxjUTh/Dc8m1U7NEzZUWka1KiP0nXnzscgP99UxdQiUjXpER/kop75/Cx0wcwZ+FmDtfooSQi0vUo0beDL0wpYd+RWv6oe9WLSBekRN8OPjS8D+OH9Gb2q+uoq9ddLUWka1GibwdmxqzzR7J59xGeWaa7WopI16JE304uHtufEYW53PfKWl1AJSJdihJ9O0mLGTOnjWD5lv28Vr4z2eGIiBylRN+Orp5QTFFeFve9sjbZoYiIHKVE346y0tO4cUoJr5fv4u2KvckOR0QESDDRm9mlZrbazMrN7PZm5p9mZn81s2oz+8cm8zaY2TtmtsTMut4Tv9vZZyYPJS87nV++VJ7sUEREgAQSvZmlAXcD04GxwLVmNrZJsd3AbcBPW1jMBe4+vqUnlEdJr+wMbpxSwgsrtrN8y75khyMiktAe/USg3N3XuXsNMAeYEV/A3Xe4+yKgtgNi7HZunFpCXnY6//XimmSHIiKSUKIvBjbHjVeE0xLlwPNmttjMZrZUyMxmmlmZmZVVVla2YfFdT35OBl+YUsJzy7ezcuv+ZIcjIikukURvzUxry4niU9x9AkHXzy1mNq25Qu4+291L3b20qKioDYvvmm6aUkJelvbqRST5Ekn0FcCQuPHBQMLPz3P3LeHfHcATBF1BkZffI4MbpgznmWXbWLVNe/UikjyJJPpFwGgzKzGzTOAaYG4iCzezXDPLaxwGLgGWnWiw3c1NU4O9+p89/26yQxGRFNZqonf3OuBW4DlgJfCouy83s1lmNgvAzAaYWQXwdeC7ZlZhZr2A/sBrZrYUWAg87e7PdlRjuprePTL50vkjeGHFdhZv1OMGRSQ5rCvel6W0tNTLyqJxyv2h6jrO/8+XGVGUyx9mTsasuUMeIiInx8wWt3QKu66M7WC5WencduEoFq7fzcvvdu+ziUSke1Ki7wTXfGgoQ/v24D+eXU1DQ9f7BSUi0aZE3wky02N845JTWLl1P3OXJnzCkohIu1Ci7yRXjhvE6YN68R/PrqKqtj7Z4YhIClGi7ySxmPEvV4xly74qfj1/XbLDEZEUokTfiSaPKGD6GQO45+W1bN9flexwRCRFKNF3sm9NH0N9g/OTZ1clOxQRSRFK9J1saEEPbpxawuNvvcfSzXuTHY6IpAAl+iS45YKRFPbM4ntzl+t0SxHpcEr0SZCXncF3Lj+NpZv38siiTckOR0QiTok+Sa4aX8zkEX35yTOr2HmwOtnhiEiEKdEniZnxw6vO4EhtPXfO04FZEek4SvRJNKpfHl/88Aj++FYFC9btSnY4IhJRSvRJ9uWPjqa4dw7feuIdXTErIh1CiT7JcjLTuPPjZ7Ku8hC/0GMHRaQDKNF3AdNOKeLvSwcze/463q7Ym+xwRCRilOi7iO9cPpaC3Ey++djb1NQ1JDscEYkQJfouIj8ngx9dfSarth3g7r+UJzscEYkQJfou5OKx/blq/CD++y/lLNHtEUSknSjRdzH/NuMM+udl8bU/LOFwTV2ywxGRCFCi72LyczL42d+PZ8OuQ/zw6ZXJDkdEIiChRG9ml5rZajMrN7Pbm5l/mpn91cyqzewf21JXPujckQXMnDaChxds4s8rtic7HBHp5lpN9GaWBtwNTAfGAtea2dgmxXYDtwE/PYG60oyvX3wKYwf24p8eW8rWfUeSHY6IdGOJ7NFPBMrdfZ271wBzgBnxBdx9h7svAmrbWleal5Wexi+vO5uauga+/PDfqK3XKZcicmISSfTFwOa48YpwWiISrmtmM82szMzKKisrE1x8tI0s6sm/f/xMyjbu4afPr052OCLSTSWS6K2ZaYk+LSPhuu4+291L3b20qKgowcVH34zxxVw3aSi/emUdL61Sf72ItF0iib4CGBI3PhjYkuDyT6auhL53xVjGDOzF1/6wlI27DiU7HBHpZhJJ9IuA0WZWYmaZwDXA3ASXfzJ1JZSdkcZ9n50AwMyHFnOoWufXi0jiWk307l4H3Ao8B6wEHnX35WY2y8xmAZjZADOrAL4OfNfMKsysV0t1O6oxUTasIJe7r5vAmh0H+MajS3HXs2ZFJDHWFRNGaWmpl5WVJTuMLun+V9fxw6dX8o2LT+HLF45Odjgi0kWY2WJ3L21uXnpnByMn56apJazYsp+fvfAuJUW5XDFuULJDEpEuTrdA6GbMjDs/cSYfGt6Hrz+6lLINu5Mdkoh0cUr03VBWehqzP1dKce8cvvhQGRt26kwcEWmZEn031Sc3k9/e8CHMjBt+u5CdB6uTHZKIdFFK9N3Y8MJcfn19Kdv2V/H5Bxayv6rpHShERJTou71zhvXhvs+ew7vbD3Dzg2VU1dYnOyQR6WKU6CPgI6f24+d/P55FG3dzy+/f0jNnReQYSvQRceVZg7hjxhm8uGoHX37kLd3tUkSOUqKPkM9OHsb3rxzLc8u369bGInKUEn3E3DClhH+9cizPLt/GbY8o2YuIEn0kfWFKCf9yxVieWbaNL/3vYh2gFUlxSvQRddPUEn509Rn8ZfUOrn9gIQd06qVIylKij7DPTBrGXZ8ez1sb93DdrxewSxdViaQkJfqImzG+mF99LjjP/uP3vsG6yoPJDklEOpkSfQq4cEx/Hpk5mQNVdXzi3jdYvFE3QhNJJUr0KWLC0D48/v/OIz8ng2t/vYAnl7yX7JBEpJMo0aeQ4YW5PP4PUzhrcD5fmbOEnzy7ivqGrvfgGRFpX0r0KaZvbia/v3ky104cyr0vr+WLD5XpZmgiEadEn4Iy02P8+9VncMdVZzD/3Uqu/OVrLHtvX7LDEpEOokSfosyMz00expyZk6mubeDj977Bwws26aHjIhGUUKI3s0vNbLWZlZvZ7c3MNzP7r3D+22Y2IW7eBjN7x8yWmJme+N3FlA7vy9O3TWVSSV++/cQ73DZnCfuOqCtHJEpaTfRmlgbcDUwHxgLXmtnYJsWmA6PD10zg3ibzL3D38S09oVySq6BnFg9+YSL/eMkpzHtnK5f94lUWrtcpmCJRkcge/USg3N3XuXsNMAeY0aTMDOAhD7wJ9Dazge0cq3SgtJhx60dH89isc0lPM66Z/Vd+/Mwq3SdHJAISSfTFwOa48YpwWqJlHHjezBab2cwTDVQ6x9lD+zDvtg/z6Q8N4b5X1nL5f73KW5v2JDssETkJiSR6a2Za0yN2xyszxd0nEHTv3GJm05pdidlMMyszs7LKysoEwpKOkpuVzp0fH8dDN06kqraBT977Bnc8tYKD1XXJDk1ETkAiib4CGBI3PhjYkmgZd2/8uwN4gqAr6APcfba7l7p7aVFRUWLRS4eadkoRz371w1w3aSgPvL6ei3/+Cs8u26Yzc0S6mUQS/SJgtJmVmFkmcA0wt0mZucD14dk3k4F97r7VzHLNLA/AzHKBS4Bl7Ri/dLC87Ax+eNWZ/DG8fcKs3y3mCw8uonyHbo4m0l20mujdvQ64FXgOWAk86u7LzWyWmc0Ki80D1gHlwK+Bfwin9wdeM7OlwELgaXd/tp3bIJ1gwtA+PPXlqXz38jEs3rCHS++azw/+tIJ9h3UqpkhXZ13xZ3hpaamXlemU+65q58Fqfvb8auYs2kx+Tga3XjCKz04eRnZGWrJDE0lZZra4pVPYdWWstFlhzyzu/Pg4nvryVMYN7s0Pn17JhT97hUfLNlOnZ9SKdDlK9HLCTh+Uz0M3TuT3N0+ioGcm33zsbS78+Ss8trhCCV+kC1HXjbQLd+eFFdu5689rWLF1P8MKejBz2gg+MWGwunREOsHxum6U6KVduTvPr9jOPX8pZ2nFPgp7ZnHj1OFcN3EovXtkJjs8kchSopdO5+78de0u7n1lLa+u2Ul2RoyPTxjMF84bzuj+eckOTyRyjpfo0zs7GEkNZsZ5owo5b1QhK7fu58HXN/DY4goeXrCJySP68plJw/jY6QPITNdhIpGOpj166TS7Dlbzh7LNPLxgExV7jlDYM5OrxhfzqdIhnDpAe/kiJ0NdN9KlNDQ489dU8sjCTby4cgd1Dc5Zg/O56uxirhg3iKK8rGSHKNLtKNFLl7XrYDX/t2QLf1xcwYqt+4kZTBlVyBXjBnLJ2AH0ydUBXJFEKNFLt7Bm+wGeXLKFJ5e+x+bdR0iLGeeNLOCS0wdw0Zh+DMzPSXaIIl2WEr10K+7O8i37efqdrTzzzlY27DoMwJnF+Xz0tH5ccFo/xhXnE4s1d3dskdSkRC/dlruztvIgz6/Yzp9XbOdvm/fiDn1zM5k6qjB4jS5kUG/t7UtqU6KXyNhzqIb5ayp5eXUlr5XvpPJANQDDC3pw7sgCJo8oYGJJX3XzSMpRopdIcndWbz/Aa2t28ua63SxYv4sDVcFTsIp753DOsD6UDu/D2UP6cNrAPDLSdM6+RJcSvaSE+gZn5db9LNqwm7KNe1i0fjc7wj3+rPQYZxTnc2bja3A+IwpzSVfyl4hQopeU5O5s2VfF3zbtYcmmvbxdsY9lW/ZxuKYeCJL/qQPyGDuwF6cOyOO0Ab04bUCeTumUbkmJXiRU3xAc3F2+ZR8rtuxn+Zb9rNy6nz1xT8oq7JnJyKKejO7fkxGFPRnZrycjCnMZ1DuHNJ3pI12U7nUjEkqLGaf0z+OU/nlcfXYwzd2pPFDNqm0HeHf7AdZsP8iaHcE5/Y19/gCZaTGGFvRgeEEPhvTtwdDwNaRvD4p755CbpX8n6Zr0yZSUZ2b065VNv17ZTDul6Oh0d2fnwRrWVR5k/c5DrN91iI07D7Nh1yHeWLvraBdQoz49MhjUO4eB+TkU985mQH4OA/OzGZCfTf9e2fTvlUWPTP3LSefTp06kBWZGUV4WRXlZTBpRcMw8d2fXoRo27T5MxZ4jVOwJ/m7de4TNuw+zYN0uDlTXfWCZeVnpFPXKoqhnsNzCnlkU9syksGcWfXMzKeiZSd/cLPr2yCQvO10XhUm7UKIXOQFmFibpLCYM7dNsmYPVdWzbV8W2fVXsOFDF9v3VbN9fReXBaioPVLN8y352Hqw+pnsoXlrM6J2TQe8eGfTukUmfHhn0yskgP+7VKzuY1is7nbzsDPKy08nLTic3K12nk8pRCSV6M7sU+AWQBtzv7j9uMt/C+ZcBh4Eb3P2tROqKRFXPrHRG9evJqH49j1uuqraeXYdq2H2whl2Hqtl9qIbdh2rYc7iGPYdr2Xu4hr2Ha9myt4qVWw+w70gtB5v5tdBUVnrsaNLvkZlOz6w0crPSyc1MJyczjR6ZafTITKdHZho5GWnkNPmblREjOyON7PQ0shuHM9LISg+GdWC6+2g10ZtZGnA3cDFQASwys7nuviKu2HRgdPiaBNwLTEqwrkhKy85Io7h3DsVtuI1DXX0D+6vqOFBVy/4jdeyvquVAOH6gqo5D1XUcrK7jQHUw3Di+51ANFXuOcLi6jsO19Ryuqaem7sQe5J4WM7LSY2Smx47+zUyLkZmeFg4bmekxMtKC6Rnh/PSYHTOcnhYjI81Ij8VIT7NjpqXFjIxwelosKJMWaxw+9m9azIjFjDR7fzwtZsQax82IxTg6LXhxtF7jePy8mBlmwS+47iyRPfqJQLm7rwMwsznADCA+Wc8AHvLgXM03zay3mQ0EhidQV0TaKD0tRt/cTPq2wzn/dfUNHKmtD1419VTVNrw/XFdPVfi3uraBqtp6qusaqKptoKY+mFZd10BNXQM19cHf6qPDwfyDVXVU1zVQW99Abb0f87euvoHaBqeuvoGGrnem91F29AsgSPrxXwgW94UQM8PC8hYOx9eLX5YZYdn3hwtys3h01rntHn8iib4Y2Bw3XkGw195ameIE6wJgZjOBmQBDhw5NICwRaQ/paTHy0mLkZWckNY76BqeuoYG6eqeu3qltaKC+IfhCCOYF0+sb/GjZBg+m1YXT6t1pCMs2hOP1DU6DO/UNwUNvGvz9cg3O0fnuHC0PHK3f4EBYxx0anLD8+/UhmNa4nKBKOBwuN5gezGxwD8scW7ZXTsccNk1kqc39Zmn63dtSmUTqBhPdZwOzIbhgKoG4RCRCgq6WNHQ5QvtLZJNWAEPixgcDWxIsk5lAXRER6UCJnH+1CBhtZiVmlglcA8xtUmYucL0FJgP73H1rgnVFRKQDtbpH7+51ZnYr8BzBKZIPuPtyM5sVzr8PmEdwamU5wemVXzhe3Q5piYiINEs3NRMRiYDj3dRMl86JiEScEr2ISMQp0YuIRJwSvYhIxHXJg7FmVglsPMHqhcDOdgynO0jFNkNqtjsV2wyp2e62tnmYuxc1N6NLJvqTYWZlLR15jqpUbDOkZrtTsc2Qmu1uzzar60ZEJOKU6EVEIi6KiX52sgNIglRsM6Rmu1OxzZCa7W63Nkeuj15ERI4VxT16ERGJo0QvIhJxkUn0Znapma02s3Izuz3Z8XQUMxtiZn8xs5VmttzMvhJO72tmL5jZmvBvn2TH2t7MLM3M/mZmT4XjqdDm3mb2mJmtCt/zc6PebjP7WvjZXmZmj5hZdhTbbGYPmNkOM1sWN63FdprZt8L8ttrMPtaWdUUi0cc9hHw6MBa41szGJjeqDlMHfMPdxwCTgVvCtt4OvOjuo4EXw/Go+QqwMm48Fdr8C+BZdz8NOIug/ZFtt5kVA7cBpe5+BsHtza8hmm1+ELi0ybRm2xn+j18DnB7WuSfMewmJRKIn7gHm7l4DND6EPHLcfau7vxUOHyD4xy8maO//hMX+B7gqKQF2EDMbDFwO3B83Oept7gVMA34D4O417r6XiLeb4DkZOWaWDvQgeCpd5Nrs7vOB3U0mt9TOGcAcd6929/UEz/6YmOi6opLoW3o4eaSZ2XDgbGAB0D98qhfh335JDK0j3AV8E2iImxb1No8AKoHfhl1W95tZLhFut7u/B/wU2ARsJXha3fNEuM1NtNTOk8pxUUn0CT+EPCrMrCfwR+Cr7r4/2fF0JDO7Atjh7ouTHUsnSwcmAPe6+9nAIaLRZdGisE96BlACDAJyzeyzyY2qSzipHBeVRJ/IA8wjw8wyCJL879398XDydjMbGM4fCOxIVnwdYArwd2a2gaBb7qNm9jui3WYIPtcV7r4gHH+MIPFHud0XAevdvdLda4HHgfOIdpvjtdTOk8pxUUn0KfMQcjMzgj7ble7+87hZc4HPh8OfB57s7Ng6irt/y90Hu/twgvf2JXf/LBFuM4C7bwM2m9mp4aQLgRVEu92bgMlm1iP8rF9IcBwqym2O11I75wLXmFmWmZUAo4GFCS/V3SPxIng4+bvAWuA7yY6nA9s5leAn29vAkvB1GVBAcJR+Tfi3b7Jj7aD2fwR4KhyOfJuB8UBZ+H7/H9An6u0G/g1YBSwD/hfIimKbgUcIjkPUEuyx33S8dgLfCfPbamB6W9alWyCIiERcVLpuRESkBUr0IiIRp0QvIhJxSvQiIhGnRC8iEnFK9CIiEadELyIScf8fKAOD2aM+HIYAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = 100\n",
        "k = 0.01\n",
        "losses = []\n",
        "w = deepcopy(W)\n",
        "for epoch in range(epochs):\n",
        "    w, loss = update_weights(x,y,w,k)\n",
        "    losses.append(loss)\n",
        "plt.plot(losses)\n",
        "plt.title('Loss over increasing number of epochs')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3llIoQD7xMJf"
      },
      "source": [
        "# Backpropagation using the Chain Rule\n",
        "\n",
        "So far we studied a way to implement **gradient descent** using brute force to estimate the gradient, which involves computing $h_i$, $f(h_i)$, $\\hat y$, and the loss for every weight and bias. This is not a very efficient way to perform backpropagation.\n",
        "\n",
        "The chain rule allows us to compute the gradient analytically, which offers a shortcut that save us several steps. The algorithm consist of calculating the gradient wrt each weight and bias in the network using the product of its partial derivatives, that is the individual components that contribute to the gradient. For example, the gradient of the loss wrt $w^h_{11}$ is:\n",
        "\n",
        "change in loss wrt $w^h_{11}$ = change in loss wrt $\\hat y$ * change in $\\hat y$ wrt $f(h_1)$ * change in $f(h_1)$ wrt $h_1$ * change in $h_1$ wrt $w^h_{11}$\n",
        "\n",
        "Using derivative notation, the expression above looks like this:\n",
        "\n",
        "$\\frac {\\delta L}{\\delta w^h_{11}} = \\frac {\\delta L}{\\delta \\hat y} \\cdot \\frac {\\delta \\hat y}{\\delta f(h_1)} \\cdot \\frac {\\delta f(h_1)}{\\delta h_1} \\cdot \\frac {\\delta h_1}{\\delta w^h_{11}}$\n",
        "\n",
        "The Figure below depicts the partial derivates contributing to the gradient of the loss wrt $w^h_{11}$.\n",
        "\n",
        "<fugure>\n",
        "    <img src=\"figures/3neuron_chain_rule.png\" width=\"500\">\n",
        "</figure>\n",
        "\n",
        "Solving the partial derivatives and simplifying (see details in pages 35-36 of the book), the equation becomes:\n",
        "\n",
        "$\\frac {\\delta L}{\\delta w^h_{11}} = -2 \\cdot (y - \\hat y) \\cdot w^o_{1} \\cdot f(h_1) \\cdot (1 - f(h_1)) \\cdot x_1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's confirm that using the chain rule yiels the same result we obtained before to update weights. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:23:28.004119Z",
          "start_time": "2020-09-24T13:23:28.001851Z"
        },
        "id": "fSSV-hAcufCd"
      },
      "outputs": [],
      "source": [
        "h = np.dot(x, w_h) + b_h\n",
        "f_h = 1/(1+np.exp(-h))\n",
        "y_hat = np.dot(f_h, w_o) + b_o\n",
        "\n",
        "# Extract individual values\n",
        "y_hat_1 = y_hat[0][0]\n",
        "y = 0 \n",
        "w_11 = w_h[0][0]\n",
        "w_o_1 = w_o[0][0]\n",
        "f_h_1 = f_h[0, 0]\n",
        "x_1 = x[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T13:23:28.008250Z",
          "start_time": "2020-09-24T13:23:28.004921Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "juDL0c_fwp0c",
        "outputId": "2cbf40fa-41ad-4b42-bc94-56b2496f2dfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Brute force w_11: -0.00487\n",
            "Chain rule w_11: -0.00487\n"
          ]
        }
      ],
      "source": [
        "# Update weights by brute force\n",
        "updated_weights, _ = update_weights(x, y, W, .01)\n",
        "print('Brute-force w_11:', np.round(updated_weights[0][0][0], 5))\n",
        "\n",
        "# Update weights using the chain rule\n",
        "w_11_updated = w_11 - (.01) * -2 * (y - y_hat_1) * (w_o_1) * f_h_1 * (1 - f_h_1) * x_1\n",
        "print('Chain rule w_11:', np.round(w_11_updated, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "updated_weights[0][0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_weights_cr(w, w_o, f_h, x, k):\n",
        "    '''Update weights using the Chain Rule.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    w: float\n",
        "        Weight to modify\n",
        "    w_o: float\n",
        "        Partial derivative of y_hat wrt f_h. \n",
        "    f_h: float\n",
        "        Activation value of the hidden layer.\n",
        "    x: int\n",
        "        Input value \n",
        "    k: float\n",
        "        Learning rate\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float \n",
        "        New weight value.\n",
        "    '''\n",
        "    return w - k * (-2 * (y - y_hat) * (w_o) * f_h * (1 - f_h) * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>w_h</th>\n",
              "      <th>w_o</th>\n",
              "      <th>f_h</th>\n",
              "      <th>update_weights</th>\n",
              "      <th>update_weights_cr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.0053</td>\n",
              "      <td>0.1528</td>\n",
              "      <td>0.589040</td>\n",
              "      <td>-0.004872</td>\n",
              "      <td>-0.004872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.5820</td>\n",
              "      <td>-0.1745</td>\n",
              "      <td>0.367792</td>\n",
              "      <td>-0.582469</td>\n",
              "      <td>-0.582469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.2723</td>\n",
              "      <td>-0.1135</td>\n",
              "      <td>0.463689</td>\n",
              "      <td>-0.272626</td>\n",
              "      <td>-0.272627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.3793</td>\n",
              "      <td>0.1528</td>\n",
              "      <td>0.589040</td>\n",
              "      <td>0.379728</td>\n",
              "      <td>0.379728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.5204</td>\n",
              "      <td>-0.1745</td>\n",
              "      <td>0.367792</td>\n",
              "      <td>-0.520869</td>\n",
              "      <td>-0.520869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.1896</td>\n",
              "      <td>-0.1135</td>\n",
              "      <td>0.463689</td>\n",
              "      <td>0.189273</td>\n",
              "      <td>0.189273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      w_h     w_o       f_h  update_weights  update_weights_cr\n",
              "0 -0.0053  0.1528  0.589040       -0.004872          -0.004872\n",
              "1 -0.5820 -0.1745  0.367792       -0.582469          -0.582469\n",
              "2 -0.2723 -0.1135  0.463689       -0.272626          -0.272627\n",
              "3  0.3793  0.1528  0.589040        0.379728           0.379728\n",
              "4 -0.5204 -0.1745  0.367792       -0.520869          -0.520869\n",
              "5  0.1896 -0.1135  0.463689        0.189273           0.189273"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Update w_h weights.\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(columns=['w_h', 'w_o', 'f_h', 'update_weights', 'update_weights_cr'])\n",
        "\n",
        "# Extract parameters.\n",
        "df['w_h'] = W[0].flatten()\n",
        "df['w_o'] = np.array([w_o, w_o]).flatten()\n",
        "df['f_h'] = np.array([f_h[0], f_h[0]]).flatten()\n",
        "\n",
        "# Calculate new weights using update_weights function (for comparison).\n",
        "df['update_weights'] = updated_weights[0].flatten()\n",
        "\n",
        "# Calculate new weights using the Chain Rule.\n",
        "df['update_weights_cr'] = df.apply(lambda row: update_weights_cr(row['w_h'], row['w_o'], row['f_h'], 1, .01)[0][0], axis=1)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Chain_rule.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('dl-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "22b4c44e3c50a171eec18a53d4a2ddea7382ccef91107100631ee18d9931920c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
